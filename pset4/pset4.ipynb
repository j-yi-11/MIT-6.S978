{"cells":[{"cell_type":"markdown","metadata":{"id":"4glFUe2R9uRc"},"source":["# **0. Load Preliminary Functions**"]},{"cell_type":"markdown","metadata":{"id":"W37lUOmT9yle"},"source":["# a. Import Libraries and Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iydqiNU5fGu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision import datasets\n","from torchvision import models\n","from torchvision import transforms\n","from torch.autograd import Variable\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.animation import FuncAnimation, PillowWriter\n","from mpl_toolkits.mplot3d import Axes3D\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import pickle\n","import itertools\n","import math\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"markdown","metadata":{"id":"Yb2COTm-ocln"},"source":["# b. MNIST Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-J1xVnVE_6Zm"},"outputs":[],"source":["tensor_transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","batch_size = 256\n","train_dataset = datasets.MNIST(root = \"./data\",\n","\t\t\t\t\t\t\t\t\ttrain = True,\n","\t\t\t\t\t\t\t\t\tdownload = True,\n","\t\t\t\t\t\t\t\t\ttransform = tensor_transform)\n","test_dataset = datasets.MNIST(root = \"./data\",\n","\t\t\t\t\t\t\t\t\ttrain = False,\n","\t\t\t\t\t\t\t\t\tdownload = True,\n","\t\t\t\t\t\t\t\t\ttransform = tensor_transform)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","\t\t\t\t\t\t\t   batch_size = batch_size,\n","\t\t\t\t\t\t\t\t shuffle = True)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","\t\t\t\t\t\t\t   batch_size = batch_size,\n","\t\t\t\t\t\t\t\t shuffle = False)\n"]},{"cell_type":"markdown","metadata":{"id":"0_5PFYwtohYY"},"source":["# **1. DDPM**\n","\n","\n","# a. Building Blocks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDICmZADIiUE"},"outputs":[],"source":["class ResidualConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.same_channels = in_channels==out_channels\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","          x1 = self.conv1(x)\n","          x2 = self.conv2(x1)\n","          if self.same_channels:\n","              out = x + x2\n","          else:\n","              out = x1 + x2\n","          return out / 1.414\n","\n","\n","class UnetDown(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetDown, self).__init__()\n","        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","class UnetUp(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UnetUp, self).__init__()\n","        layers = [\n","            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n","            ResidualConvBlock(out_channels, out_channels),\n","            ResidualConvBlock(out_channels, out_channels),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, skip):\n","        x = torch.cat((x, skip), 1)\n","        x = self.model(x)\n","        return x\n","\n","\n","class EmbedFC(nn.Module):\n","    def __init__(self, input_dim, emb_dim):\n","        super(EmbedFC, self).__init__()\n","        self.input_dim = input_dim\n","        layers = [\n","            nn.Linear(input_dim, emb_dim),\n","            nn.GELU(),\n","            nn.Linear(emb_dim, emb_dim),\n","        ]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.input_dim)\n","        return self.model(x)\n","\n","\n","class ContextUnet(nn.Module):\n","    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n","        super(ContextUnet, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.n_feat = n_feat\n","        self.n_classes = n_classes\n","\n","        self.init_conv = ResidualConvBlock(in_channels, n_feat)\n","\n","        self.down1 = UnetDown(n_feat, n_feat)\n","        self.down2 = UnetDown(n_feat, 2 * n_feat)\n","\n","        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n","\n","        self.timeembed1 = EmbedFC(1, 2*n_feat)\n","        self.timeembed2 = EmbedFC(1, 1*n_feat)\n","        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n","        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n","\n","        self.up0 = nn.Sequential(\n","            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7),\n","            nn.GroupNorm(8, 2 * n_feat),\n","            nn.ReLU(),\n","        )\n","\n","        self.up1 = UnetUp(4 * n_feat, n_feat)\n","        self.up2 = UnetUp(2 * n_feat, n_feat)\n","        self.out = nn.Sequential(\n","            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n","            nn.GroupNorm(8, n_feat),\n","            nn.ReLU(),\n","            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n","        )\n","\n","    def forward(self, x, c, t, context_mask):\n","        x = self.init_conv(x)\n","        down1 = self.down1(x)\n","        down2 = self.down2(down1)\n","        hiddenvec = self.to_vec(down2)\n","\n","        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n","\n","        context_mask = context_mask[:, None]\n","        context_mask = context_mask.repeat(1,self.n_classes)\n","        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n","        c = c * context_mask\n","\n","        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n","        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n","        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n","        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n","\n","        up1 = self.up0(hiddenvec)\n","        up2 = self.up1(cemb1*up1+ temb1, down2)\n","        up3 = self.up2(cemb2*up2+ temb2, down1)\n","        out = self.out(torch.cat((up3, x), 1))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"9yAENstIAuDh"},"source":["# b. DDPM Schedules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16Wd4uXlAsE7"},"outputs":[],"source":["def ddpm_schedules(beta1, beta2, T):\n","    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n","\n","    alpha_t = 0.0\n","    oneover_sqrta = 0.0\n","    sqrt_beta_t = 0.0\n","    alphabar_t= 0.0\n","    sqrtab = 0.0\n","    sqrtmab = 0.0\n","    mab_over_sqrtmab_inv  = 0.0\n","    ##################\n","    ### Problem 1 (a): Implement ddpm_schedules()\n","    ##################\n","    ##################\n","\n","    return {\n","        \"alpha_t\": alpha_t,  # \\alpha_t\n","        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n","        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n","        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n","        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n","        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n","        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n","    }"]},{"cell_type":"markdown","metadata":{"id":"IeL8cLF-A9u-"},"source":["# c. DDPM Main Module\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQb0JijTA9CQ"},"outputs":[],"source":["class DDPM(nn.Module):\n","    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n","        super(DDPM, self).__init__()\n","        self.nn_model = nn_model.to(device)\n","\n","        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n","            self.register_buffer(k, v)\n","\n","        self.n_T = n_T\n","        self.device = device\n","        self.drop_prob = drop_prob\n","        self.loss_mse = nn.MSELoss()\n","\n","    def forward(self, x, c):\n","        ##################\n","        ### Problem 1 (b): Implement ddpm_schedules()\n","        ##################\n","        ##################\n","        return 0.0\n","\n","    def sample(self, n_sample, size, device, guide_w=0.0):\n","        x_i = torch.randn(n_sample, *size).to(device)\n","        c_i = torch.arange(0, 10).to(device)\n","        c_i = c_i.repeat(n_sample // c_i.shape[0])\n","\n","        context_mask = torch.zeros_like(c_i).to(device)\n","\n","        c_i = c_i.repeat(2)\n","        context_mask = context_mask.repeat(2)\n","        context_mask[n_sample:] = 1.0  # second half context-free\n","\n","        for i in range(self.n_T, 0, -1):\n","            print(f'Sampling timestep {i}', end='\\r')\n","\n","            ##################\n","            ### Problem 1 (b): Implement ddpm_schedules()\n","            ##################\n","            ##################\n","        return x_i\n"]},{"cell_type":"markdown","metadata":{"id":"XYUg7AjHottB"},"source":["# c. Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrG8n5WJAo51"},"outputs":[],"source":["def train_main_loop(ddpm, optim, dataloader, lr,\n","                    ws_test, n_epoch, batch_size):\n","    for ep in range(n_epoch):\n","        print(f'epoch {ep}')\n","        ddpm.train()\n","\n","        # linear lrate decay\n","        optim.param_groups[0]['lr'] = lr*(1-ep/n_epoch)\n","\n","        pbar = tqdm(dataloader)\n","        loss_ema = None\n","        # train\n","        for x, c in pbar:\n","            optim.zero_grad()\n","            x = x.to(device)\n","            c = c.to(device)\n","            loss = ddpm(x, c)\n","            loss.backward()\n","            if loss_ema is None:\n","                loss_ema = loss.item()\n","            else:\n","                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n","            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n","            optim.step()\n","\n","        # eval\n","        ddpm.eval()\n","        with torch.no_grad():\n","            n_sample = 20\n","            for w_i, w in enumerate(ws_test):\n","                x_gen = ddpm.sample(n_sample, (1, 28, 28), device, guide_w=w)\n","\n","            fig, ax = plt.subplots(5, 4, figsize=(6, 6))\n","            for i, j in itertools.product(range(5), range(4)):\n","                ax[i,j].get_xaxis().set_visible(False)\n","                ax[i,j].get_yaxis().set_visible(False)\n","\n","            for k in range(n_sample):\n","                i = k//4\n","                j = k%4\n","                ax[i,j].cla()\n","                ax[i,j].imshow(x_gen[k,:].data.cpu().numpy().reshape(28, 28), cmap='Greys')\n","            plt.show()\n","\n","        if ep == n_epoch - 1:\n","          plt.savefig('final_generated_samples.png', bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{"id":"Il39DdgeBPKc"},"source":["# e. Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1-Pnrj8BTqr"},"outputs":[],"source":["# hardcoding these here\n","n_epoch = 20\n","n_T = 400\n","n_feat = 128\n","lr = 1e-4\n","ws_test = [0.0, 0.5, 2.0]\n","\n","ddpm = DDPM(nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=10), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n","ddpm.to(device)\n","\n","tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n","\n","optim = torch.optim.Adam(ddpm.parameters(), lr=lr)\n","train_main_loop(ddpm, optim, train_loader, lr, ws_test, n_epoch, batch_size)\n"]},{"cell_type":"markdown","metadata":{"id":"P48jRvRiuqYt"},"source":["# **2. Ablation Study**"]},{"cell_type":"code","source":["##################\n","### Problem 2: Ablation Study\n","##################"],"metadata":{"id":"10QVE-I3V4IP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **3. Continuous-time Stochastic Process**"],"metadata":{"id":"5BZ70JTmXzE2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIYYxtkArxKy"},"outputs":[],"source":["timesteps = 1000\n","beta1 = 0.1\n","beta2 = 50.0\n","dt = 1.0 / timesteps\n","means = np.array([0.5, -0.5])\n","stds = np.array([0.02, 0.02])\n","weights = np.array([0.5, 0.5])\n","weights /= np.sum(weights)\n","n_samples = 100\n","x_min = -3\n","x_max = 3\n","x_grid = np.linspace(x_min, x_max, num=200)\n","\n","def get_beta_t(t):\n","    ratio = float(t) / timesteps\n","    return ratio * beta2 + (1 - ratio) * beta1\n","\n","def f(x, t):\n","    beta_t = get_beta_t(t)\n","    return -0.5 * beta_t * x\n","\n","def g(t):\n","    beta_t = get_beta_t(t)\n","    return np.sqrt(beta_t)\n","\n","def gaussian_pdf(x, mean, std):\n","    return (1.0 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std) ** 2)\n","\n","def mixture_pdf(x):\n","    pdf = np.zeros_like(x)\n","    for i in range(len(means)):\n","        pdf += weights[i] * gaussian_pdf(x, means[i], stds[i])\n","    return pdf\n","\n","def sample_mixture_gaussian(n_samples):\n","    components = np.random.choice(len(means), size=n_samples, p=weights / np.sum(weights))\n","    samples = np.random.normal(loc=means[components], scale=stds[components])\n","    return samples\n","\n","def p_xt(x_t, t):\n","    p_xt_val = np.zeros_like(x_t)\n","    ##################\n","    ### Problem 3(a): p(x(t))\n","    ##################\n","    ##################\n","    return p_xt_val\n","\n","def grad_log_p_xt(x_t, t):\n","    ##################\n","    ### Problem 3(b): \\nabla_x(t) \\log p(x(t))\n","    ##################\n","    ##################\n","    return 1.0\n","\n","def forward_sde(timesteps, n_samples, dt):\n","    x = np.zeros((timesteps, n_samples))\n","\n","    x_pdf = np.zeros((timesteps, x_grid.shape[0]))\n","\n","    x0 = sample_mixture_gaussian(n_samples)\n","    x0_pdf = mixture_pdf(x_grid)\n","    x[0] = x0\n","    x_pdf[0] = x0_pdf\n","\n","    for t in range(1, timesteps):\n","        noise = np.random.normal(0, 1, size=n_samples)\n","        x[t] = x[t-1] + f(x[t-1], t) * dt + g(t) * noise * np.sqrt(dt)\n","        x_pdf[t] = p_xt(x_grid, t)\n","\n","    return x, x_pdf\n","\n","def backward_sde(timesteps, n_samples, dt):\n","    x = np.zeros((timesteps, n_samples))\n","    xT = np.random.normal(0, 1, size=n_samples)\n","    x[-1] = xT\n","\n","    for t in range(timesteps - 1, 0, -1):\n","        noise = np.random.normal(0, 1, size=n_samples)\n","        delta_x = (f(x[t], t) - g(t) ** 2 * grad_log_p_xt(x[t], t)) * dt + g(t) * noise * np.sqrt(dt)\n","        x[t-1] = x[t] + delta_x\n","\n","    return x\n","\n","forward_x, forward_x_pdf = forward_sde(timesteps, n_samples, dt)\n","backward_data = backward_sde(timesteps, n_samples, dt)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(36, 12))\n","\n","for i in range(n_samples):\n","    axes[0].plot(forward_x[:, i], lw=1)\n","\n","time = np.arange(timesteps)\n","X, Y = np.meshgrid(time, x_grid)\n","pcm = axes[0].pcolormesh(X, Y, forward_x_pdf.T,\n","                         cmap='viridis', shading='auto', vmin=0.0, vmax=forward_x_pdf.max())\n","axes[0].set_title('Forward SDE')\n","axes[0].set_xlabel('Timesteps')\n","axes[0].set_ylabel('x')\n","axes[0].set_ylim([x_min, x_max])\n","\n","for i in range(n_samples):\n","    axes[1].plot(backward_data[::-1][:, i], lw=1)\n","\n","X, Y = np.meshgrid(time, x_grid)\n","pcm = axes[1].pcolormesh(X, Y, forward_x_pdf[::-1].T,\n","                         cmap='viridis', shading='auto', vmin=0.0, vmax=forward_x_pdf.max())\n","axes[1].set_title('Reverse SDE')\n","axes[1].set_xlabel('Timesteps')\n","axes[1].set_ylabel('x')\n","\n","plt.tight_layout()\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}