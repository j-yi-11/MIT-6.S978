{"cells":[{"cell_type":"markdown","metadata":{"id":"4glFUe2R9uRc"},"source":["# **0. Load Preliminary Functions**"]},{"cell_type":"markdown","metadata":{"id":"W37lUOmT9yle"},"source":["# a. Import Libraries and Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iydqiNU5fGu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.autograd import Variable\n","\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import numpy as np\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import pickle\n","import itertools\n","import math\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"markdown","metadata":{"id":"Yb2COTm-ocln"},"source":["# b. MNIST Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-J1xVnVE_6Zm"},"outputs":[],"source":["tensor_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","\t\ttransforms.Normalize(mean=(0.5, ), std=(0.5, ))\n","])\n","\n","batch_size = 128\n","train_dataset = datasets.MNIST(root = \"./data\",\n","\t\t\t\t\t\t\t\t\ttrain = True,\n","\t\t\t\t\t\t\t\t\tdownload = True,\n","\t\t\t\t\t\t\t\t\ttransform = tensor_transform)\n","test_dataset = datasets.MNIST(root = \"./data\",\n","\t\t\t\t\t\t\t\t\ttrain = False,\n","\t\t\t\t\t\t\t\t\tdownload = True,\n","\t\t\t\t\t\t\t\t\ttransform = tensor_transform)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","\t\t\t\t\t\t\t   batch_size = batch_size,\n","\t\t\t\t\t\t\t\t shuffle = True)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","\t\t\t\t\t\t\t   batch_size = batch_size,\n","\t\t\t\t\t\t\t\t shuffle = False)\n"]},{"cell_type":"markdown","metadata":{"id":"0_5PFYwtohYY"},"source":["# **1. Unconditional GAN**\n","\n","\n","# a. Training Function for Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDICmZADIiUE"},"outputs":[],"source":["criterion = nn.BCELoss()\n","\n","def train_discriminator(discriminator, d_optimizer, images, real_labels, fake_images, fake_labels, with_condition, cls_labels):\n","    discriminator.zero_grad()\n","    if with_condition:\n","      outputs = discriminator(images, cls_labels)\n","    else:\n","      outputs = discriminator(images)\n","    real_loss = criterion(outputs, real_labels.view(-1, 1))\n","    real_score = outputs\n","\n","    if with_condition:\n","      outputs = discriminator(fake_images, cls_labels)\n","    else:\n","      outputs = discriminator(fake_images)\n","\n","    fake_loss = criterion(outputs, fake_labels.view(-1, 1))\n","    fake_score = outputs\n","\n","    d_loss = real_loss + fake_loss\n","    d_loss.backward()\n","    d_optimizer.step()\n","    return d_loss, real_score, fake_score, fake_loss"]},{"cell_type":"markdown","metadata":{"id":"9yAENstIAuDh"},"source":["# b. Training Function for Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16Wd4uXlAsE7"},"outputs":[],"source":["def train_generator(generator, g_optimizer, discriminator_outputs, real_labels, with_condition):\n","    generator.zero_grad()\n","    g_loss = criterion(discriminator_outputs, real_labels.view(-1, 1))\n","    g_loss_fake = criterion(discriminator_outputs, 1 - real_labels.view(-1, 1))\n","    g_loss.backward()\n","    g_optimizer.step()\n","    return g_loss, g_loss_fake"]},{"cell_type":"markdown","metadata":{"id":"IeL8cLF-A9u-"},"source":["# c. Monitors for Images and Losses\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQb0JijTA9CQ"},"outputs":[],"source":["def monitor_images(generator, test_noise, with_condition):\n","    num_test_samples = test_noise.shape[0]\n","    size_figure_grid = int(math.sqrt(num_test_samples))\n","    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n","    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n","        ax[i,j].get_xaxis().set_visible(False)\n","        ax[i,j].get_yaxis().set_visible(False)\n","\n","    if with_condition:\n","      cls_labels = torch.randint(0, 10, (num_test_samples,)).to(device)\n","      test_images = generator(test_noise, label = cls_labels)\n","    else:\n","      test_images = generator(test_noise)\n","\n","    for k in range(num_test_samples):\n","        i = k//4\n","        j = k%4\n","        ax[i,j].cla()\n","        ax[i,j].imshow(test_images[k,:].data.cpu().numpy().reshape(28, 28), cmap='Greys')\n","        if with_condition:\n","            ax[i, j].set_title(f'Class: {cls_labels[k].item()}', fontsize=10)\n","    plt.show()\n","\n","\n","def monitor_losses(d_losses, d_losses_fake, g_losses, g_losses_fake):\n","    def moving_average(x, win=100):\n","        return np.convolve(x, np.ones(win), 'same') / np.convolve(np.ones_like(x), np.ones(win), 'same')\n","\n","    plt.figure(figsize=(10, 5))\n","    iters = np.arange(len(d_losses))\n","    epochs = iters * batch_size / 60000\n","    plt.plot(epochs, moving_average(d_losses), label='d_loss')\n","    plt.plot(epochs, moving_average(g_losses), label='g_loss')\n","    plt.plot(epochs, moving_average(d_losses_fake), label='d_loss_fake')\n","    plt.plot(epochs, moving_average(g_losses_fake), label='g_loss_fake')\n","    plt.xlabel('epochs')\n","    plt.ylabel('loss')\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"XYUg7AjHottB"},"source":["# c. Model for Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrG8n5WJAo51"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, channels = [512, 256, 128], with_condition=False):\n","        super().__init__()\n","        self.with_condition = with_condition\n","        self.model = None\n","\n","        ##################\n","        ### Problem 1: Implement Discriminator\n","        ##################\n","\n","    def forward(self, x, label=None):\n","        x = x.view(x.size(0), 784)\n","        if self.with_condition:\n","          assert label is not None\n","          ##################\n","          ### Problem 3: Implement Conditional GAN\n","          ##################\n","        out = self.model(x)\n","        out = out.view(out.size(0), -1)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Il39DdgeBPKc"},"source":["# e. Model for Generator\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1-Pnrj8BTqr"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, dim_z=100, channels = [128, 256, 512], with_condition=False):\n","        super().__init__()\n","        self.dim_z = dim_z\n","        self.with_condition = with_condition\n","\n","        self.model = None\n","        ##################\n","        ### Problem 1: Implement Generator\n","        ##################\n","\n","    def forward(self, x, label=None):\n","        x = x.view(x.size(0), self.dim_z)\n","\n","        if self.with_condition:\n","          assert label is not None\n","          ##################\n","          ### Problem 3: Implement Conditional GAN\n","          ##################\n","\n","        out = self.model(x)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"S2KxDaEBo-9Z"},"source":["# d. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySZrtUoglEug"},"outputs":[],"source":["num_batches = len(train_loader)\n","\n","def train_main_loop(generator, discriminator, d_optimizer, g_optimizer, test_noise,\n","                    num_epochs, d_freq, with_condition):\n","  d_losses = []\n","  g_losses = []\n","  d_losses_fake = []\n","  g_losses_fake = []\n","  for epoch in range(num_epochs):\n","      for n, (images, cls_labels) in enumerate(train_loader):\n","          images = Variable(images.to(device))\n","          real_labels = Variable(torch.ones(images.size(0)).to(device))\n","          cls_labels = cls_labels.to(device)\n","\n","          # Sample from generator\n","          noise = Variable(torch.randn(images.size(0), dim_z).to(device))\n","          fake_images = generator(noise, label=cls_labels if with_condition else None)\n","          fake_labels = Variable(torch.zeros(images.size(0)).to(device))\n","\n","          # Train the discriminator\n","          d_loss, real_score, fake_score, d_loss_fake = train_discriminator(discriminator, d_optimizer, images, real_labels, fake_images, fake_labels, with_condition, cls_labels=cls_labels)\n","\n","          if n % d_freq == 0:\n","            # Sample again from the generator and get output from discriminator\n","            noise = Variable(torch.randn(images.size(0), dim_z).to(device))\n","            fake_images = generator(noise, label=cls_labels if with_condition else None)\n","            outputs = discriminator(fake_images, label=cls_labels if with_condition else None)\n","\n","            # Train the generator\n","            g_loss, g_loss_fake = train_generator(generator, g_optimizer, outputs, real_labels, with_condition)\n","\n","          d_losses.append(d_loss.data.detach().cpu().numpy())\n","          g_losses.append(g_loss.data.detach().cpu().numpy())\n","          d_losses_fake.append(d_loss_fake.data.detach().cpu().numpy())\n","          g_losses_fake.append(g_loss_fake.data.detach().cpu().numpy())\n","\n","          if (n+1) % 100 == 0:\n","              print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, d_loss_fake: %.4f, g_loss: %.4f, g_loss_fake: %.4f, '\n","                    'D(x): %.2f, D(G(z)): %.2f'\n","                    %(epoch + 1, num_epochs, n+1, num_batches, d_loss.data, d_loss_fake.data, g_loss.data, g_loss_fake.data,\n","                      real_score.data.mean(), fake_score.data.mean()))\n","\n","      monitor_images(generator, test_noise, with_condition=with_condition)\n","      monitor_losses(d_losses, d_losses_fake, g_losses, g_losses_fake)\n","  return d_losses, d_losses_fake, g_losses, g_losses_fake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_n_2PGRUAzou"},"outputs":[],"source":["# set number of epochs and initialize figure counter\n","num_epochs = 500\n","dim_z = 100\n","\n","num_test_samples = 16\n","test_noise = Variable(torch.randn(num_test_samples, dim_z).to(device))\n","d_freq = 1\n","\n","with_condition = False\n","\n","discriminator = Discriminator(with_condition=with_condition).to(device)\n","generator = Generator(with_condition=with_condition).to(device)\n","\n","lr = 0.0002\n","d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n","g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n","\n","d_losses, d_losses_fake, g_losses, g_losses_fake = train_main_loop(generator, discriminator, d_optimizer, g_optimizer, test_noise, num_epochs, d_freq, with_condition)\n","monitor_images(generator, test_noise, with_condition=with_condition)\n","monitor_losses(d_losses, d_losses_fake, g_losses, g_losses_fake)"]},{"cell_type":"markdown","source":["# e. Ablation Study"],"metadata":{"id":"THi9HQP1m5wZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDRSZeovtYa_"},"outputs":[],"source":["##################\n","### Problem 2: Ablation study 1, 2, and 3\n","##################\n"]},{"cell_type":"markdown","metadata":{"id":"P48jRvRiuqYt"},"source":["# **2. Conditional GAN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIYYxtkArxKy"},"outputs":[],"source":["num_epochs = 500\n","dim_z = 100\n","\n","num_test_samples = 16\n","test_noise = Variable(torch.randn(num_test_samples, dim_z).to(device))\n","d_freq = 1\n","\n","with_condition = True\n","\n","discriminator = Discriminator(with_condition=with_condition).to(device)\n","generator = Generator(with_condition=with_condition).to(device)\n","\n","lr = 0.0002\n","d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n","g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n","\n","d_losses, d_losses_fake, g_losses, g_losses_fake = train_main_loop(generator, discriminator, d_optimizer, g_optimizer, test_noise, num_epochs, d_freq, with_condition)\n","monitor_images(generator, test_noise, with_condition=with_condition)\n","monitor_losses(d_losses, d_losses_fake, g_losses, g_losses_fake)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}